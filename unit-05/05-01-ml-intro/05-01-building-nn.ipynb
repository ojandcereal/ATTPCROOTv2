{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building and training a simple neural network\n",
        "\n",
        "In this notebook we will construct the neural net seen in the videos by Sanderson. For this, we will use the python library [PyTorch](https://pytorch.org/) and the [MINST](http://deeplearning.net/data/mnist/) dataset discussed in the video. This notebook is based loosly on a [PyTorch tutorial](https://pytorch.org/tutorials/beginner/nn_tutorial.html) by Jeremy Howard.\n",
        "\n",
        "My goal for this notebook is for you to start to see the connections between everything we have done in the class so far, and neural networks. You have already learned nearly all of the math needed to understand these networks! All that's needed is seeing how it all fits together in this new paradigm.\n",
        "\n",
        "Run the cell below to import all of the python packages we will require in this notebook."
      ],
      "metadata": {
        "id": "YR4G-yQdiQw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path # internal python library for managing file paths\n",
        "import requests # used to doqload files using web requests\n",
        "import pickle\n",
        "import gzip # used to unzip files\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from torch import nn\n",
        "import copy"
      ],
      "metadata": {
        "id": "EM7Iw80_jAtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting and displaying the MINST dataset"
      ],
      "metadata": {
        "id": "6ZJ-yH3ri0ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will start by creating a folder on Google's servers to hold the data in the MINT dataset."
      ],
      "metadata": {
        "id": "fD70g6ygjqKY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_5YgKEjiP19"
      },
      "outputs": [],
      "source": [
        "data_path = Path(\"data\")\n",
        "path = data_path/\"mnist\" # this defines the file path as data/mnist\n",
        "path.mkdir(parents=True, exist_ok=True) # this creates the folders \"data\" and \"data/mnist\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will download the data (only if we haven't downloaded it before), and save it in the folder we created in the last code cell."
      ],
      "metadata": {
        "id": "5NoW4rxaj9MR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_url = \"https://github.com/pytorch/tutorials/raw/main/_static/\"\n",
        "data_filename = \"mnist.pkl.gz\"\n",
        "\n",
        "if not (data_path / data_filename).exists():\n",
        "  content = requests.get(data_url + data_filename).content\n",
        "  (data_path/data_filename).open(\"wb\").write(content)"
      ],
      "metadata": {
        "id": "BXEbW1mEkmkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is stored as a numpy array. It was saved using the python library \"pickle\" in a python-specific data format. In the following code cell we will unpack the data and then store the numpy arrays in the following variables:\n",
        "\n",
        "- `image_train`: This is a numpy array that contains all of the images in the training dataset. We will use this to train our neural network.\n",
        "- `label_train`: This is a numpy array that contains the corresponding label for each image in the training dataset. We will use this to train our neural network.\n",
        "- `image_valid`: This is a numpy array that contains all of the images in the validation dataset. After training our network we will use this to evaluate how well our model does on data it has not seen before.\n",
        "- `label_valid`: This is a numpy array that contains the corresponding label for each image in the validation dataset. After training our network we will use this to evaluate how well our model does on data it has not seen before.\n",
        "\n",
        "**Run the following code cell to unpack and load all of the data into these four variables.**"
      ],
      "metadata": {
        "id": "snNczCm6lLiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with gzip.open((data_path / data_filename).as_posix(), \"rb\") as unpacked_data:\n",
        "  ((image_train, label_train), (image_valid, label_valid), _) = pickle.load(unpacked_data, encoding=\"latin-1\")"
      ],
      "metadata": {
        "id": "eL9k7MRxlJsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the data is loaded and unpacked, we can display some of the data. Because the data is stored in a single 784 element array, we need to re-shape it into the 28x28 black and white image. This is done using the `reshape(x_pixels, y_pixels)` function. In our data, each pixel is a single number ranging from 0 (black) to 1 (white)\n",
        "\n",
        "Try running the code below loading in a different image by changing the `elem = 0` line. **What is the first element of the array `image_train` that is a `9`?**"
      ],
      "metadata": {
        "id": "nWRNpmgHmQAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "elem = 0 # This variable corresponds which picture we want to view\n",
        "print(f\"The image is an example of the number {label_train[elem]}\")\n",
        "image = image_train[elem].reshape((28, 28)) # create the image from the raw data\n",
        "plt.imshow(image, cmap=\"gray\") # Show the image in gray-scale (each pixel is represented by a single number from 0 to 1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cbVsdiR0mV5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Building a neural network\n"
      ],
      "metadata": {
        "id": "rxokNQ9uoVdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a number of components that go into building a neural network, all of which were discussed in the two videos by Sanderson. The primary components are listed below and we will work on building each of them piece by piece:\n",
        "\n",
        "- **Cost function**: This is the function we are trying to minimize. In our case, it is the squared difference between the predictions of the model (the value of the neurons in the final layer) and the expected value.\n",
        "- **Activation function**: This is the sigmoid or logistic function we use to \"squish\" the value of each neuron into a value between 0 and 1.\n",
        "- **Network (or model) structure**: This will define the different layers of the network. In our case, we have an input layer, two hidden layers each with 16 neurons, and one output layer."
      ],
      "metadata": {
        "id": "mKppk9wEpTmg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the cost function\n",
        "\n",
        "To start we will define the cost function and we will use the same cost function as Sanderson in the video. Our cost function is the sum of the square of the difference between the predicted and actual value for each nueron in the output layer. It is the same as the cost function we used when talking about emperical models, where we take the difference between the data and the model, and then square it.\n",
        "\n",
        "We could could this cost function ourselves, but PyTorch implements it already. `MSELoss` is a function that stands for \"Mean squared error loss.\" What Sanderson (and I) are calling the \"cost function\" is most often called the \"loss function\" in the world of machine learning, hence the word loss in the name. Regardless of its name, it is the same as the square of the residuals we saw when studying empirical models and is the function that we are trying to minimize when training our model.\n",
        "\n",
        "While we are at it, we will also define a function to generate the target. The target is what we expect the model to output. In our case this is a list of numbers that are all zero, except for the element that corresponds to the didgit in the image. For example, if the image corresponds to a `0` then the target would be `[1,0,0,0,0,0,0,0,0,0]` and if the image was a `5` then the target would be `[0,0,0,0,0,5,0,0,0,0]`.\n",
        "\n",
        "**What would the target be for an image of a `3`?**\n",
        "\n",
        "\n",
        "**What would you expect the cost function to return if the two inputs where `input = [0, 0.5, 1, 0, 0, 0, 0, 0, 0, 0]` and `target = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]`?**"
      ],
      "metadata": {
        "id": "VoHzuSxqsP8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the cost function that takes in the model prediction (input) and the expected output by the model (target)\n",
        "def cost_func(input, target):\n",
        "   cost = torch.nn.MSELoss(reduction='sum')\n",
        "   return cost(input, target)\n",
        "\n",
        "# Define a function that takes in a digit (0-9) and creates the corresponding model target\n",
        "def get_target(digits):\n",
        "  target = F.one_hot(torch.tensor(digits), 10)\n",
        "  return target\n"
      ],
      "metadata": {
        "id": "5KjoNfurtI3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check your answers to the questions above by running the code cell below."
      ],
      "metadata": {
        "id": "tTVoKlKrq9Df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target = get_target(3)\n",
        "print(f\"The target when the image is 3 is {target.numpy()}\")\n",
        "\n",
        "input = torch.tensor([0, 0.5, 1, 0, 0, 0, 0, 0, 0, 0])\n",
        "target = torch.tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "print(f\"The cost function is {cost_func(input, target)}\")"
      ],
      "metadata": {
        "id": "RQ24BYS8rRA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the cost function"
      ],
      "metadata": {
        "id": "Kshiyr_93mCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test if our cost function is working as we would expect. We will start by creating a random [tensor](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor) (essentially just an `np.array` or normal python list but with some additional information needed by PyTorch). Because there are 10 possible digits, we will create a tensor with 10 elements."
      ],
      "metadata": {
        "id": "JFQ1512qzlCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tensor to act as our output with random values\n",
        "out = torch.rand(10)\n",
        "print(f\"Our simulated random output from the model is {out.numpy()}.\")"
      ],
      "metadata": {
        "id": "5aLewLh4zkZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will create the target tensor. This should be a tensor where every element is 0 except for the element that corresponds to the digit being displayed. The code below displays the digit and prints the corresponding target tensor. You can play around with which element we are displaying and see how the target tensor changes."
      ],
      "metadata": {
        "id": "--UA5Jm00L92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "elem = 0 # Element in the training set to display\n",
        "print(f\"The image is an example of the number {label_train[elem]}\")\n",
        "\n",
        "# Create a tensor representing the true value. All values are zero except for the element corresponding to the correct digit\n",
        "tar = get_target(label_train[elem])\n",
        "print(f\"Our expected output from the model is {tar.numpy()}.\")\n",
        "\n",
        "# Display the image corresponding to the prediction tensor\n",
        "image = image_train[elem].reshape((28, 28))\n",
        "plt.imshow(image, cmap=\"gray\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "11YUXI1N0qOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can actually test our cost function. **Does the value of the cost function make sense given the predicted and target tensors?**"
      ],
      "metadata": {
        "id": "UIxCFRYl2SFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Predicted tensor: {out.numpy()}\")\n",
        "print(f\"Target tensor: {tar.numpy()}.\")\n",
        "cost = cost_func(out,tar)\n",
        "print(f\"The value of the cost function is {cost.item():.6f}\")"
      ],
      "metadata": {
        "id": "vvR0iNHh1Lqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the activation function\n",
        "\n",
        "Now we need to define the activation function, or how we \"squish\" the value in each neuron . In this case we will use the sigmoid function:\n",
        "\n",
        "$$\\frac{1}{1+e^{-x}}$$\n",
        "\n",
        " Like the cost function, this can be done using existing functions in PyTorch"
      ],
      "metadata": {
        "id": "U3kLyPy-rUfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "activation_func = F.sigmoid"
      ],
      "metadata": {
        "id": "C2E-4jLcpTHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the activation function"
      ],
      "metadata": {
        "id": "QyWUsAjs4J8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The activation function should make large negative numbers be close to 0 and large positive numbers be close to 1. Play around with the values in the `input` tensor below (you can also add more values) to see how the function works."
      ],
      "metadata": {
        "id": "gWw0KEJM4NQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.tensor([0, 100, -100])\n",
        "\n",
        "for input_val in input:\n",
        "  print(f\"The sigmoid of {input_val} is {activation_func(input_val)}.\")"
      ],
      "metadata": {
        "id": "A5AuYGkqnGCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the model\n",
        "\n",
        "The final step is now creating the shape of the model. At first we will emulate the model that Sanderson used in his video. That model consisted of one input layer, two hidden layers, and one output layer. The layers had the following sizes:\n",
        "\n",
        "- Input layer: 728 elements, each corresponding to the brightness of a single pixel in an image.\n",
        "- Hidden layer 1: 16 elements\n",
        "- Hidden layer 2: 16 elements\n",
        "- Output layer: 10 elements, each one corresponding to how sure the model is that the input image is a particular digit."
      ],
      "metadata": {
        "id": "XXe_uVuV5jqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will create a new python class to represent our model based on an existing PyTorch class. Most of the details here are not important. The comments in the code below will highlight the important bits for us (what are the sizes of the different layers, and where are they created)."
      ],
      "metadata": {
        "id": "pDDfh_zV7AYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NN_Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.hidden1 = nn.Linear(784,16) # Our input layer has 784 neurons and our first hidden layer has 16 elements\n",
        "    self.hidden2 = nn.Linear(16,16) # Our second hidden layer has 16 inputs and 16 outputs\n",
        "    self.output = nn.Linear(16,10) # Our output layer has 16 inputs and 10 outputs (because there are 10 digits)\n",
        "\n",
        "  def forward(self, xb):\n",
        "    # xb is the value of all the neurons at the current layer.\n",
        "    # At the start it is a tensor with 784 elements\n",
        "\n",
        "    # Do the matrix multiplication and then take the sigmoid of each element in the first hidden layer.\n",
        "    xb = activation_func(self.hidden1(xb))\n",
        "    # xb is now a tensor with 16 elements\n",
        "\n",
        "    # Do the matrix multiplication and then take the sigmoid of each element in the second hidden layer.\n",
        "    xb = activation_func(self.hidden2(xb))\n",
        "    # xb is now a tensor with 16 elements\n",
        "\n",
        "    # Do the matrix multiplication and then take the sigmoid of each element in the output layer.\n",
        "    xb = activation_func(self.output(xb))\n",
        "    # xb is now a tensor with 10 elements\n",
        "\n",
        "    return xb"
      ],
      "metadata": {
        "id": "bRXzXBvF6JBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How could we modify the model above to add a third hidden layer with 16 neurons (do not actually modify the code)?**"
      ],
      "metadata": {
        "id": "v3ndPfybr1aX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the model"
      ],
      "metadata": {
        "id": "brPrqNTFBpNh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code cell below will create a new version of our model, randomly setting the weights and biases each time the code is run. We will then grab the first element in our training set (the 5 we have seen multiple times) and run the image through the model. Finally, we print the output (prediction) of the model, what the model should give, and the value of the cost function.\n",
        "\n",
        "**Does the model prediction change each time you run the code cell? Why?**"
      ],
      "metadata": {
        "id": "tXI9sq1xDdnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = NN_Model() # This will default to creating a model with random weights and biases.\n",
        "\n",
        "# We will grab the first image in the training set\n",
        "input = torch.tensor(image_train[0])\n",
        "\n",
        "# and now create the corresponding target\n",
        "target = get_target(label_train[0])\n",
        "\n",
        "# Run the model on the input\n",
        "output = model(input)\n",
        "print(f\"The ouput of the untrained model is {output.detach().numpy()}.\")\n",
        "print(f\"The target value is {target.numpy()}.\")\n",
        "print(f\"The cost function is {cost_func(output, target)}.\")\n"
      ],
      "metadata": {
        "id": "ethXNZDw-07s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's it! You have now sucessfully built and ran a neural network for classifying hand written digits!! There's just one glaring problem, it does a terrible job. Our next step will be to train the model, or adjust all of the 13000 values that make up the model until our model is actually useful for identifying digits."
      ],
      "metadata": {
        "id": "9L3C3ELsElbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model"
      ],
      "metadata": {
        "id": "iD-zdFK1ENFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's now time to train the model. Because this is a process we will likely want to do numerous times, we will define a python function for it. The training process involves the following steps (**can you map these steps to the code below?**)\n",
        "\n",
        "1. We are intrested in studying how well the model can identify digits as we continue to train the model. As a result, we will create empty lists to save how well the model is doing after each iteration of training using all of the data (or **epoch**). We will also save a copy of the model after each epoch.\n",
        "2. If we were doing a pure gradient descent method, we would evaluate the model for every instance of the training set, then apply the gradient descent method to adjust all of the parameters in our model. This would be unreasonably slow and computationally infeasible. Instead, we only run the model on some small subset of data (a **batch**). After each batch, we then apply the gradient descent method. As a result we apply the gradient descent method many times in a single epoch. If you're intrested in more details for why this works and why it's a good idea, check out the [third video in Sanderson's series](https://www.youtube.com/watch?v=Ilg3gGewQ5U). Trusting this is a good idea that works, for each batch we then:\n",
        "  1. Get the start and end indices of the batch we are processing.\n",
        "  1. Get the subset of data (`xb`) to run through the model to get our predictions from the output layer (`pred`). Get the subset of labels (`yb`) to compare to our predicitons.\n",
        "  1. Evaluate the cost function for all of the training examples in our batch\n",
        "  1. Run backpropagation to calculate the gradient of the cost function (and how much we should update each parameter).\n",
        "  1. Adjust each parameter value in the model based on the value calculated in the previous step.\n",
        "  1. Reset internal variables used to calcualte the gradient of the batch.\n",
        "1. Once we have completed this for every batch in an epoch we then evaluate the average cost function for the entrire training set and store this in our list to return to the user.\n",
        "1. Repeat steps 2 and 3 until we have trained the model on all of the data multiple times.\n",
        "\n",
        "The code below implements these steps. In fact, it is general enough that we can change the structure of the model, or the data, and still be able to use the same training function. In this class, any time we need to train a model, I will give you this function or a version similar to it."
      ],
      "metadata": {
        "id": "sKbSxZybGUdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will define a function for training out model:\n",
        "# epochs is the number of times we pass over the complete training set.\n",
        "# batch_size is how many training examples to combine before updating the model parameters\n",
        "# learning_rate controls how large of a step we take each time we update the model parameters\n",
        "# data_train is the list of images we are training on (the entirety of this dataset corresponds to a single epoch)\n",
        "# label_train is the list of labels corresponding to the images in data_train\n",
        "# model is the model we are training\n",
        "\n",
        "# The function will return a list containing the average cost function after each epoch\n",
        "def train_model(epochs, batch_size, learning_rate, data_train, label_train, model):\n",
        "\n",
        "  # Create lists to return\n",
        "  epoch_data = []\n",
        "  cost_train = []\n",
        "  models = []\n",
        "\n",
        "\n",
        "  # We will repeat the training process multiple times, printing the current average of the cost function after each epoch\n",
        "  for epoch in range (epochs):\n",
        "\n",
        "    # Each iteration of this loop will evaluate the model\n",
        "    # on a subset of the data containing batch_size images\n",
        "    for i in range((len(data_train) - 1) // batch_size + 1):\n",
        "      # Get the array indices to process this batch\n",
        "      i_start = i * batch_size\n",
        "      i_end = i_start + batch_size\n",
        "\n",
        "      # get the predictions and targets for this batch\n",
        "      xb = torch.tensor(data_train[i_start:i_end])\n",
        "      yb = get_target(label_train[i_start:i_end])\n",
        "      pred = model(xb)\n",
        "      cost = cost_func(pred, yb.float())\n",
        "\n",
        "      # Update the model parameters (i.e. train the model) for this batch of data\n",
        "      cost.backward() # This calculates the rate of change using backpropogation\n",
        "      with torch.no_grad():\n",
        "        for p in model.parameters():\n",
        "          # THIS IS THE SAME AS OUR NORMAL DYNAMIC MODEL CODE!\n",
        "          rate = -p.grad # get the rate of change for this model parameter\n",
        "          p += rate * learning_rate # update this model parameter using our rate of change and \"timestep\"\n",
        "        model.zero_grad() # Reset the rate of change, ready for the next batch\n",
        "\n",
        "    # End of batch_size loop\n",
        "\n",
        "    xb = torch.tensor(data_train)\n",
        "    yb = get_target(label_train)\n",
        "    pred = model(xb)\n",
        "\n",
        "    # calculate the average cost function\n",
        "    cost = cost_func(pred, yb) / len(pred)\n",
        "\n",
        "    print(f\"After epoch {epoch} the cost function is {cost}\")\n",
        "    epoch_data.append(epoch)\n",
        "    cost_train.append(cost.item())\n",
        "    models.append(copy.deepcopy(model))\n",
        "\n",
        "  return epoch_data, cost_train, models\n"
      ],
      "metadata": {
        "id": "RkdWOWm9EO4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## So how is this at all useful anyway?\n",
        "\n",
        "There is a lot of fairly complicated math that underlies the process of training a machine learning model, but you've seen it all before! I don't want to get too stuck on the details. Instead, I want to focus on the big picture of how we can effect the training process and how we can tell if this whole machine learning thing is doing something reasonable. Are we actually building something that can properly identify digits?\n",
        "\n",
        "To start to answer that question, let's actually take our data and start training a model. To train our model we need the following pieces:\n",
        "1. The model to train (our `NN_Model` class in this example)\n",
        "1. Our data we will use to train our model (`image_train` and `label_train`)\n",
        "1. Select the **hyperparameters** used to train the model\n",
        "  - The **learning rate** corresponds to how *large of a step* we take each time we update the model parameters (this is like our timestep `dt` from the previous unit).\n",
        "  - The **batch size** is the number of training examples to group together before updating the model parameters.\n",
        "  - The **number of epochs** is the number of times we will pass over all of the data while training.\n",
        "\n",
        "Run the following code to train the model using some default hyperparameters:"
      ],
      "metadata": {
        "id": "b7y77TpYjBGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters for training\n",
        "learning_rate = 0.05\n",
        "batch_size = 100\n",
        "num_epochs = 10\n",
        "\n",
        "# This will default to creating a model with random weights and biases.\n",
        "model = NN_Model()\n",
        "\n",
        "# Train the data. Save the average cost function and the model after each epoch\n",
        "epochs, costs, models = train_model(num_epochs, batch_size, learning_rate, image_train, label_train, model)\n",
        "\n",
        "# plot the cost function as a function of epoch\n",
        "fig = plt.figure(figsize=(8,6))\n",
        "plt.title(\"Cost function vs. training epoch\")\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Cost function (arb)')\n",
        "plt.grid(which='both', axis='both')\n",
        "plt.plot(epochs, costs, 'r-', label=\"Training set\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Lk74yF6xNwE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If everything is working as expected, what should the cost function do as we train the model on more and more epochs?**\n"
      ],
      "metadata": {
        "id": "PZ7ux9BQnyer"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increase the number of epochs we are training for and rerun the code (remember it takes about a second to run an epoch). **What do you notice about the behavior of the cost function as you train for larger number of epochs?**"
      ],
      "metadata": {
        "id": "qBk5C94Eo6qH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Effect of hyperparameters\n",
        "\n",
        "Rerun the code cell above (for 10 epochs) with the following changes. For each of the changes record the value you set the hyperparameter to and answer the question: **How does changing the hyperparameter impact the training?**\n",
        "\n",
        "- Decrease the `learning_rate`:\n",
        "- Increase the `learning_rate`:\n",
        "- Decrease the `batch_size`:\n",
        "- Increase the `batch_size`:\n",
        "\n",
        "1. **How large can you make the learning rate before the training breaks? What do you think is happening when the training breaks?**\n",
        "\n",
        "2. **What is the learning rate in the context of what we learned last unit (that is what did we call it last unit)? How did we see the same behavior at large learning rate last unit?**\n",
        "\n",
        "\n",
        "**Return the hyperparameters to values where the model is being trained well, and re-run the code to train the model before continuing.**"
      ],
      "metadata": {
        "id": "hkcywclt4O7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## So how well can we identify digits?\n",
        "\n",
        "Well, the cost function is consistantly going down as we continue to train. That's a good sign, but what does it mean for the percentage of the data we can actually identify? Let's explore that.\n",
        "\n",
        "We will start by defining a function get a single number that corresponds to the digit being identified by the model."
      ],
      "metadata": {
        "id": "n0CsXVD5sl7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_digit(pred):\n",
        "  return torch.argmax(pred, dim=0)"
      ],
      "metadata": {
        "id": "KcXLelZdtVP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's print some images of digits, along with the guesses. Setting the value of `valid` will choose if we should display an example where the model was correct or not. The variable `elem` chooses where we start searching through the data for the next example that meets the condition selected in `valid`.\n",
        "\n",
        "**Print a few cases where the model is wrong and a few cases where the model is correct. Does it make sense to you that those images are the ones where the model is incorrect? Why or why not?**\n"
      ],
      "metadata": {
        "id": "YigseUbV7Yx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "elem = 0 # Element in the training set to display\n",
        "\n",
        "# if True will show the first example where the model is correct\n",
        "# if False will show the first example where the model is wrong\n",
        "valid = True\n",
        "\n",
        "# Look for the first value of elem that matches the required condition\n",
        "while ( get_digit(model(torch.tensor(image_train[elem]))) == label_train[elem] ) != valid:\n",
        "  elem = elem + 1\n",
        "\n",
        "\n",
        "print(f\"The image at {elem} is an example of the number {label_train[elem]}\")\n",
        "digit = get_digit(model(torch.tensor(image_train[elem])))\n",
        "print(f\"The model identified the image as a {digit}.\")\n",
        "\n",
        "# Display the image corresponding to the prediction tensor\n",
        "image = image_train[elem].reshape((28, 28))\n",
        "plt.imshow(image, cmap=\"gray\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dqDtVkoK7Mdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will start by defining a function to calculate the **accuracy** of our model, or what percentage of the data is being properly identified."
      ],
      "metadata": {
        "id": "CdVxc-ML7NO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model: model to evaluate the accuracy of\n",
        "# data: numpy arrays containing the images to run through the model\n",
        "# labels: correct labels for our images\n",
        "def accuracy(model, data, labels):\n",
        "  xb = torch.tensor(data)\n",
        "  yb = get_target(labels)\n",
        "  pred = model(xb)\n",
        "\n",
        "  num_right = 0\n",
        "  for i in range(pred.shape[0]):\n",
        "    guess = get_digit(pred[i])\n",
        "    if guess == labels[i]:\n",
        "      num_right = num_right + 1\n",
        "\n",
        "  return num_right / pred.shape[0]"
      ],
      "metadata": {
        "id": "rnRF-MqR7J-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What do expect the accuracy of the model to be if we were to evaluate it on a model with random weights?**\n",
        "\n",
        "After writing down your prediction, check it with the code below.\n"
      ],
      "metadata": {
        "id": "R8ORkFa7xBji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rand_acc = accuracy(NN_Model(), image_train, label_train)\n",
        "print(f\"With a model with random weights the accuracy is {rand_acc}\")"
      ],
      "metadata": {
        "id": "zmx742bPyM8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Re-train the model for a large number of epochs (say 100).** Do this by re-running the cell above that trains the model and outputs the loss curve. In our training function, we saved and returned the state of the model after each epoch. Let's evaluate and plot the model accuracy as a function of epoch. **What do you notice about the behavior of the accuracy as you train for larger number of epochs?**"
      ],
      "metadata": {
        "id": "F84HlVtJxJZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = []\n",
        "for model in models:\n",
        "  acc.append(accuracy(model, image_train, label_train))\n",
        "\n",
        "print(f\"With our best model, our accuracy is {acc[-1]*100:.2f}%\")\n",
        "\n",
        "fig = plt.figure(figsize=(8,6))\n",
        "plt.title(\"Accuracy vs. training epoch\")\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(which='both', axis='both')\n",
        "plt.plot(epochs, acc, 'r-', label=\"Training set\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bubPVyx4vIeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What's the best we can do?\n",
        "\n",
        "It would appear from the behavior of the cost function, and the accuracy, that we just keep getting better as we train longer and longer. When we are calculating the the cost function and accuracy using just the training data, that's usually true. Over time we will just keep getting closer and closer to a minimum.\n",
        "\n",
        "But if our model is to have any use at all it has to be able to identify *all* digits, not just the exact examples we showed it while training. This is a common problem in machine learning, and to account for it we always split the data we have to train the model up into two groups. First is the training group (what we have been using so far). The second is the **validation group**. This is a set of labeled data that is not used in training. Instead we use it to evaluate how well the model does on *new* data it has not seen during training. If you recall, the MINST dataset comes with the data pre-split.\n",
        "\n",
        "The code below plots the accuracy as a function of model for both the training and validation data sets. **What do you notice about the accuracy as a function of epoch? How is the behavior different for both data sets?**\n",
        "\n",
        "**What is the best accuracy you would feel comfortable claiming for your model? Why?**"
      ],
      "metadata": {
        "id": "RfASexF5sjVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_train = []\n",
        "acc_valid = []\n",
        "for model in models:\n",
        "  acc_train.append(accuracy(model, image_train, label_train))\n",
        "  acc_valid.append(accuracy(model, image_valid, label_valid))\n",
        "\n",
        "print(f\"With our best model, our accuracy is {acc_train[-1]*100:.2f}% on the training set and {acc_valid[-1]*100:.2f}% on the validation set\")\n",
        "\n",
        "fig = plt.figure(figsize=(8,6))\n",
        "plt.title(\"Accuracy vs. training epoch\")\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(which='both', axis='both')\n",
        "plt.plot(epochs, acc_train, 'r-', label=\"Training set\")\n",
        "plt.plot(epochs, acc_valid, 'b-', label=\"Validation set\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3GG8akbRzMPp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}